{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZzis2H1xC6iVZmclS0blr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/softmurata/Image2VideoML/blob/main/animate_anything.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyXn4Vz8Qxpq"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/alibaba/animate-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/animate-anything/\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "qQrq3LB3RBou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cloudbook-public-production.oss-cn-shanghai.aliyuncs.com/animation/animate_anything_512_v1.02.tar"
      ],
      "metadata": {
        "id": "fwQ97EzZRZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/animate-anything/animate_anything_512_v1.02.tar"
      ],
      "metadata": {
        "id": "DNUggIMORiTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"/content/animate-anything/output/latent/\", exist_ok=True)\n",
        "!mv /content/animate-anything/animate_anything_512_v1.02 /content/animate-anything/output/latent/"
      ],
      "metadata": {
        "id": "E3wkzrTDUFJZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/animate-anything\n",
        "!python train.py --config output/latent/animate_anything_512_v1.02/config.yaml --eval validation_data.prompt_image=example/barbie2.jpg validation_data.prompt='A cartoon girl is talking.'"
      ],
      "metadata": {
        "id": "4aa_2XeFSYAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('/content/animate-anything/output/demo/barbie2/0.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "cC8LcluuWfwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Samples"
      ],
      "metadata": {
        "id": "ixnFn2SnXsZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cdn-ak.f.st-hatena.com/images/fotolife/t/touch-sp/20231218/20231218161528.jpg -O /content/concat.jpg"
      ],
      "metadata": {
        "id": "VzUq5PbNXued"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def split_image_horizontally(image_path, output_path1, output_path2):\n",
        "    # 画像を読み込み\n",
        "    original_image = Image.open(image_path)\n",
        "\n",
        "    # 画像のサイズを取得\n",
        "    width, height = original_image.size\n",
        "\n",
        "    # 画像を横方向に2分割\n",
        "    left_image = original_image.crop((0, 0, width // 2, height))\n",
        "    right_image = original_image.crop((width // 2, 0, width, height))\n",
        "\n",
        "    # 分割された画像を保存\n",
        "    left_image.save(output_path1)\n",
        "    right_image.save(output_path2)\n",
        "\n",
        "# 例: 画像を横方向に2分割して保存\n",
        "input_image_path = \"/content/concat.jpg\"\n",
        "output_image_path1 = \"/content/rgb.jpg\"\n",
        "output_image_path2 = \"/content/mask.jpg\"\n",
        "\n",
        "split_image_horizontally(input_image_path, output_image_path1, output_image_path2)\n"
      ],
      "metadata": {
        "id": "4qP0VdGSX1M4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/rgb.jpg /content/animate-anything/example/rgb.jpg\n",
        "!cp /content/mask.jpg /content/animate-anything/example/rgbmask.jpg"
      ],
      "metadata": {
        "id": "UFo_ynTHYluq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config.yaml\n",
        "\"\"\"\n",
        "pretrained_model_path: output/latent/animate_anything_512_v1.02\n",
        "output_dir: ./output/latent\n",
        "train_data:\n",
        "  width: 512\n",
        "  height: 512\n",
        "  use_bucketing: false\n",
        "  return_mask: true\n",
        "  return_motion: true\n",
        "  sample_start_idx: 1\n",
        "  fps: 8\n",
        "  n_sample_frames: 16\n",
        "  json_path: /webvid/animation0.json\n",
        "validation_data:\n",
        "  prompt: closeup face photo of japanese woman in black clothes, night city street, bokeh, fireworks in background\n",
        "  prompt_image: example/rgb.jpg\n",
        "  mask: example/rgbmask.jpg\n",
        "  sample_preview: true\n",
        "  num_frames: 16\n",
        "  fps: 4\n",
        "  width: 512\n",
        "  height: 512\n",
        "  num_inference_steps: 25\n",
        "  guidance_scale: 9\n",
        "dataset_types:\n",
        "- json\n",
        "shuffle: true\n",
        "validation_steps: 200\n",
        "trainable_modules:\n",
        "- all\n",
        "- attn1\n",
        "- conv_in\n",
        "- temp_conv\n",
        "- motion\n",
        "not_trainable_modules: []\n",
        "trainable_text_modules: null\n",
        "extra_unet_params: null\n",
        "extra_text_encoder_params: null\n",
        "train_batch_size: 4\n",
        "max_train_steps: 10000\n",
        "learning_rate: 5.0e-06\n",
        "scale_lr: false\n",
        "lr_scheduler: constant\n",
        "lr_warmup_steps: 0\n",
        "adam_beta1: 0.9\n",
        "adam_beta2: 0.999\n",
        "adam_weight_decay: 0\n",
        "adam_epsilon: 1.0e-08\n",
        "max_grad_norm: 1.0\n",
        "gradient_accumulation_steps: 1\n",
        "gradient_checkpointing: true\n",
        "text_encoder_gradient_checkpointing: false\n",
        "checkpointing_steps: 2000\n",
        "resume_from_checkpoint: null\n",
        "resume_step: null\n",
        "mixed_precision: fp16\n",
        "use_8bit_adam: false\n",
        "enable_xformers_memory_efficient_attention: false\n",
        "enable_torch_2_attn: true\n",
        "seed: null\n",
        "train_text_encoder: false\n",
        "use_offset_noise: false\n",
        "rescale_schedule: false\n",
        "offset_noise_strength: 0.1\n",
        "extend_dataset: false\n",
        "cache_latents: false\n",
        "cached_latent_dir: null\n",
        "lora_version: cloneofsimo\n",
        "save_lora_for_webui: true\n",
        "only_lora_for_webui: false\n",
        "lora_bias: none\n",
        "use_unet_lora: false\n",
        "use_text_lora: false\n",
        "unet_lora_modules:\n",
        "- UNet3DConditionModel\n",
        "text_encoder_lora_modules:\n",
        "- CLIPEncoderLayer\n",
        "save_pretrained_model: true\n",
        "lora_rank: 16\n",
        "lora_path: ''\n",
        "lora_unet_dropout: 0.1\n",
        "lora_text_dropout: 0.1\n",
        "logger_type: tensorboard\n",
        "motion_mask: true\n",
        "motion_strength: true\n",
        "kwargs: {}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BUOMa7iEeove"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/animate-anything\n",
        "# train.py: 1037 -> mask = Image.open(validation_data.mask).convert(\"L\")\n",
        "!python train.py --config configs/config.yaml --eval"
      ],
      "metadata": {
        "id": "dWWy_824Yfyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('/content/animate-anything/output/demo/rgb/3.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "uAkycBbBel01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wedding"
      ],
      "metadata": {
        "id": "I98rj3HIfD92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# マスクを生成してからのpipelineを見せつけたい"
      ],
      "metadata": {
        "id": "di_1jWYRnws-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForUniversalSegmentation, AutoProcessor\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"shi-labs/oneformer_ade20k_swin_large\"\n",
        "model = AutoModelForUniversalSegmentation.from_pretrained(model_id).to(device)\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUuC6qnKtWyw",
        "outputId": "6cb3220c-7580-49c2-c8fe-a7216f1772a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:417: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "img_path = \"/content/animate-anything/example/wedding.jpg\"\n",
        "target_name = \"person\"\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "semantic_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\").to(device)\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(**semantic_inputs)\n",
        "\n",
        "semantic_segmentation = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
        "semantic_segmentation = semantic_segmentation.detach().cpu().numpy()\n",
        "\n",
        "target_id = model.config.label2id[target_name]\n",
        "\n",
        "label_pred_map = np.where(semantic_segmentation == target_id, 255, 0)\n",
        "label_pred_map = 255 - label_pred_map\n",
        "cv2.imwrite(\"/content/animate-anything/example/weddingmask.jpg\", label_pred_map)\n",
        "display(Image.fromarray(label_pred_map.astype(np.uint8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "uN_f1mqLticQ",
        "outputId": "633fa5ed-d100-45bd-f4d2-9bd49cc8c617"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=458x458>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with mask\n",
        "%cd /content/animate-anything\n",
        "!python train.py --config configs/wedding_mask.yaml --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_tazSVOuEso",
        "outputId": "500eb490-5f42-4aa3-aa4c-dc5e65bd7ba8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/animate-anything\n",
            "2023-12-26 11:18:50.030632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-26 11:18:50.030681: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-26 11:18:50.035698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-26 11:18:51.305792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Initializing the conversion map\n",
            "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
            "motion mask True, motion_strength True\n",
            "33 Attention layers using Scaled Dot Product Attention.\n",
            "  4% 1/25 [00:03<01:30,  3.78s/it]/content/animate-anything/models/pipeline.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  motion = torch.tensor(motion, device=device)\n",
            "100% 25/25 [01:38<00:00,  3.93s/it]\n",
            "save file output/demo/wedding/0.mp4, motion strength 3 -> 4.0078125, motion precision 0.9146206545290588\n",
            "  4% 1/25 [00:03<01:32,  3.84s/it]/content/animate-anything/models/pipeline.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  motion = torch.tensor(motion, device=device)\n",
            "100% 25/25 [01:38<00:00,  3.93s/it]\n",
            "save file output/demo/wedding/1.mp4, motion strength 4 -> 4.21484375, motion precision 0.9096778575509503\n",
            "100% 25/25 [01:38<00:00,  3.94s/it]\n",
            "save file output/demo/wedding/2.mp4, motion strength 5 -> 6.01953125, motion precision 0.9137611389160156\n",
            "100% 25/25 [01:38<00:00,  3.94s/it]\n",
            "save file output/demo/wedding/3.mp4, motion strength 6 -> 6.5234375, motion precision 0.9142683849074277\n",
            "100% 25/25 [01:38<00:00,  3.94s/it]\n",
            "save file output/demo/wedding/4.mp4, motion strength 7 -> 6.2734375, motion precision 0.9137611389160156\n",
            "example/wedding.jpg precision 0.9132178349638936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('/content/animate-anything/output/demo/wedding/1.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "EoWMFzv3w98Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zexy.net/contents/wedding/wonderful/0103/images/AA1_1.jpg -O /content/animate-anything/example/wedding.jpg"
      ],
      "metadata": {
        "id": "bUOxoYd5fE65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zexy.net/pa/7770008920/01/2275785_458.jpg -O /content/animate-anything/example/wedding.jpg"
      ],
      "metadata": {
        "id": "wTnHgr9FknwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/animate-anything\n",
        "!python train.py --config configs/wedding.yaml --eval"
      ],
      "metadata": {
        "id": "LULw7HugfU6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open('/content/animate-anything/output/demo/wedding/1.mp4', 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"100%\" height=\"100%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "qy7zWiOehwaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}